{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68fa27bb-8b40-49e7-a400-58a36221b977",
   "metadata": {},
   "source": [
    " # Build Generative AI Assistant with RAG, LLMs, and Multimodal Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a69c61d-db39-4aeb-9c06-c16b674e5600",
   "metadata": {},
   "source": [
    "Building the Retrieval Layer with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785c1e97-959a-49c4-893d-82acc5ac0297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS is a library for efficient similarity search.\n",
      "Machine learning models require high‑quality training data.\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained model for embeddings\n",
    "# Sample corpus\n",
    "# Generate embeddings\n",
    "# Initialize FAISS index\n",
    "# Query\n",
    "# Search for top 2 most similar documents\n",
    "# Print retrieved documents\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np \n",
    "\n",
    "# Load a pre-trained model for embeddings\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Sample corpus\n",
    "\n",
    "corpus = [\n",
    "    \"Artificial intelligence is transforming many industries.\",\n",
    "    \"FAISS is a library for efficient similarity search.\",\n",
    "    \"The weather today is sunny with light winds.\",\n",
    "    \"Machine learning models require high‑quality training data.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "\n",
    "corpus_embeddings = model.encode(corpus, convert_to_numpy=True)\n",
    "\n",
    "# Initialize FAISS index\n",
    "embedding_dim = corpus_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(corpus_embeddings)\n",
    "\n",
    "# Query\n",
    "\n",
    "query = \"How do we search for similar documents?\"\n",
    "\n",
    "# Encode query\n",
    "\n",
    "query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "\n",
    "# Search for top 2 most similar documents\n",
    "\n",
    "distances, retrieved_indices = index.search(query_embedding, k=2)\n",
    "\n",
    "# Print retrieved documents\n",
    "\n",
    "for idx in retrieved_indices[0]:\n",
    "    print(corpus[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a233739-30c4-4633-a50f-332ceb54c958",
   "metadata": {},
   "source": [
    "Integrating LLM for Response Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f909486e-196d-4a7d-84f9-eb81b7327e10",
   "metadata": {},
   "source": [
    "Designing Structured Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52a1c63-184c-4bf3-ad0f-bbdb699af8b6",
   "metadata": {},
   "source": [
    "Implement structured output generation for consistent data formatting. Steps:\n",
    "\n",
    "Create a structured response dictionary with the following keys:\n",
    " *  \"question\": the query from Task 1\n",
    " *  \"context\": one of the retrieved documents from Task 1  \n",
    " *  \"response\": a sample response about how LLMs work\n",
    "\n",
    "Convert to JSON with proper formatting\n",
    "\n",
    "Print the structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ef45d0a-1b24-4c64-a54c-970e21def9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb986a51a19b43519eaff27c6cc5e940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a87d2371754bb4ba679db5ee3f993f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7430446d7d924ce393c27df763f48284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3afa0f1164486d8d176bd82d72de73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a3ff95945d4022ab5f4cfb0cd02935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82aeadda85524b218293f03f6be96206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7009f315dcb45fd82590139763c5eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAG Prompt ===\n",
      "Use the context to answer the question.\n",
      "\n",
      "Context:\n",
      "FAISS is a library for efficient similarity search.\n",
      "Vector databases store embeddings for fast retrieval.\n",
      "\n",
      "Question: How do we search for similar documents?\n",
      "Answer:\n",
      "\n",
      "=== Model Response ===\n",
      "Vector databases store embeddings for fast retrieval\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Example corpus\n",
    "\n",
    "corpus = [\n",
    "    \"FAISS is a library for efficient similarity search.\",\n",
    "    \"Machine learning models require high-quality training data.\",\n",
    "    \"Transformers are neural networks designed for sequence modeling.\",\n",
    "    \"Vector databases store embeddings for fast retrieval.\"\n",
    "]\n",
    "\n",
    "# 2. Encode corpus using SBERT\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_numpy=True)\n",
    "\n",
    "# 3. Build FAISS index\n",
    "\n",
    "dimension = corpus_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(corpus_embeddings)\n",
    "\n",
    "# 4. User query + retrieval\n",
    "\n",
    "query = \"How do we search for similar documents?\"\n",
    "query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
    "\n",
    "k = 2\n",
    "distances, retrieved_indices = index.search(query_embedding, k)\n",
    "\n",
    "retrieved_context = \"\\n\".join([corpus[i] for i in retrieved_indices[0]])\n",
    "\n",
    "# 5. Load FLAN‑T5 (instruction model)\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# 6. GOOD RAG PROMPT\n",
    "\n",
    "prompt = (\n",
    "    f\"Use the context to answer the question.\\n\\n\"\n",
    "    f\"Context:\\n{retrieved_context}\\n\\n\"\n",
    "    f\"Question: {query}\\n\"\n",
    "    f\"Answer:\"\n",
    ")\n",
    "\n",
    "# 7. Tokenize\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# 8. Generate\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=150,\n",
    "    num_beams=5\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=== RAG Prompt ===\")\n",
    "print(prompt)\n",
    "print(\"\\n=== Model Response ===\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a549f89-f079-43b3-a356-218ae33e2d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"question\": \"How do we search for similar documents?\",\n",
      "    \"context\": \"FAISS is a library for efficient similarity search.\",\n",
      "    \"response\": \"Large Language Models (LLMs) work by predicting the next word in a sequence based on patterns learned from massive text datasets. They use transformer architectures to capture context and generate coherent, human-like text.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Create a structured response dictionary\n",
    "response_data = {\n",
    "    \"question\": query,\n",
    "    \"context\": corpus[retrieved_indices[0][0]],   # take the top retrieved document\n",
    "    \"response\": \"Large Language Models (LLMs) work by predicting the next word in a sequence based on patterns learned from massive text datasets. They use transformer architectures to capture context and generate coherent, human-like text.\"\n",
    "}\n",
    "\n",
    "# Convert to JSON with proper formatting\n",
    "structured_output = json.dumps(response_data, indent=4)\n",
    "\n",
    "# Print the structured output\n",
    "print(structured_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc78b9-5ee7-4748-b09c-9cca6b7819d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
