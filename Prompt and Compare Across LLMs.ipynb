{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97007275-ab22-4a24-ba83-ad96178c0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0e6fd1-e847-400a-ad1b-796f002cf8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Model 1: DistilGPT-2 (lightweight GPT-2 variant)\n",
    "model1_name = \"distilgpt2\"\n",
    "generator1 = pipeline('text-generation', model=model1_name, max_length=150)\n",
    "\n",
    "# Model 2: GPT-2 small\n",
    "model2_name = \"gpt2\"\n",
    "generator2 = pipeline('text-generation', model=model2_name, max_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e186fbb-1781-48ae-b9ee-c3862f02e32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Local models loaded successfully!\n",
      "Model 1: distilgpt2\n",
      "Model 2: gpt2\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ Local models loaded successfully!\")\n",
    "print(f\"Model 1: {model1_name}\")\n",
    "print(f\"Model 2: {model2_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bce4fc3-7105-41d0-b7f4-1d99f381fcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilGPT-2: Describe Newton's first law was that the state had to decide on its own. It was the idea that a state was an autonomous system, and that the state had to decide on its own.\n",
      "\n",
      "\n",
      "\n",
      "But Newton's law was more\n",
      "GPT-2 small: Describe Newton's first law of motion to me. I am a student of Newton's law of motion. I am sure you will understand that Newton is right that the motion of the earth is proportional to the velocity of the sun. (The law\n"
     ]
    }
   ],
   "source": [
    "# Simple test prompt\n",
    "prompt = \"Describe Newton's first law\"\n",
    "\n",
    "# Generate text with DistilGPT-2\n",
    "output1 = generator1(prompt, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95, min_length=80)\n",
    "\n",
    "# Generate text with GPT-2 small\n",
    "output2 = generator2(prompt, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95, min_length=80)\n",
    "\n",
    "print(f\"DistilGPT-2: {output1[0]['generated_text']}\")\n",
    "print(f\"GPT-2 small: {output2[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "377e29af-494e-4285-8d95-3a6217b4c12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing baseline prompt comparison...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DistilGPT-2:\n",
      "While these technologies may be useful to the human body, they may not be as effective for the human body as they are for the environment.\n",
      "\n",
      "\n",
      "\n",
      "In a new study of the human body, the researchers found that the benefits of these technologies are not completely dependent on the environment. The study suggests that they may be better than the ones which were produced by the humans. And they find that they are better than those which were produced by the humans.\n",
      "The study was published in\n",
      "--------------------------------------------------\n",
      "\n",
      "GPT-2:\n",
      "A great example of this is the way in which environmental sustainability is connected to business.\n",
      "\n",
      "Crowdsourcing can be a powerful tool to help companies understand the benefits of sustainability. The benefits of large scale social-engineering are huge.\n",
      "\n",
      "The way that sustainability is tied to business is through a system of management and engagement that enables companies to make strategic decisions about how they spend their effort.\n",
      "\n",
      "This system of management can be effective in helping businesses develop new processes for\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define baseline prompt\n",
    "baseline_prompt = \"Explain the concept of sustainability in simple terms.\"\n",
    "\n",
    "def compare_models(prompt, temperature=0.7, max_tokens=100):\n",
    "    \"\"\"Compare responses from multiple models\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Model 1 response\n",
    "    try:\n",
    "        response1 = generator1(prompt, max_length=len(prompt.split()) + max_tokens, \n",
    "                              temperature=temperature, do_sample=True, pad_token_id=50256)\n",
    "        results['DistilGPT-2'] = response1[0]['generated_text'][len(prompt):].strip()\n",
    "    except Exception as e:\n",
    "        results['DistilGPT-2'] = f\"Error: {e}\"\n",
    "    \n",
    "    # Model 2 response\n",
    "    try:\n",
    "        response2 = generator2(prompt, max_length=len(prompt.split()) + max_tokens, \n",
    "                              temperature=temperature, do_sample=True, pad_token_id=50256)\n",
    "        results['GPT-2'] = response2[0]['generated_text'][len(prompt):].strip()\n",
    "    except Exception as e:\n",
    "        results['GPT-2'] = f\"Error: {e}\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the function\n",
    "print(\"üîç Testing baseline prompt comparison...\")\n",
    "baseline_results = compare_models(baseline_prompt)\n",
    "\n",
    "for model, response in baseline_results.items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"{response}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
